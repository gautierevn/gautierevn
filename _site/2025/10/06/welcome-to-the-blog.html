<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WACV 2025 · Gautier Evennou</title>
    <style>
      :root {
        color-scheme: light;
      }

      *, *::before, *::after {
        box-sizing: border-box;
      }

      body {
        margin: 0;
        font-family: "Inter", "Segoe UI", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
        background: #f6f3ed;
        color: #1f1f1f;
        line-height: 1.7;
        text-rendering: optimizeLegibility;
      }

      a {
        color: #2856b6;
        text-decoration: none;
        transition: color 0.2s ease;
      }

      a:hover {
        color: #1c3f83;
        text-decoration: underline;
      }

      body {
        background: #fbf7f0;
      }

      header {
        padding: 2.25rem clamp(1rem, 4vw, 3rem);
        border-bottom: 1px solid rgba(31, 31, 31, 0.07);
        background: #fbf7f0;
      }

      .site-identity {
        display: flex;
        align-items: baseline;
        justify-content: space-between;
        gap: 1rem;
        flex-wrap: wrap;
        max-width: 960px;
        margin: 0 auto;
      }

      header h1 {
        margin: 0;
        font-size: 1.5rem;
        font-weight: 600;
        letter-spacing: -0.01em;
        color: #1f3f9a;
      }

      .site-tagline {
        margin: 0;
        color: #615b54;
        font-size: 0.95rem;
      }

      main {
        width: min(760px, calc(100% - 3rem));
        margin: 0 auto 4rem;
        padding: 0 clamp(0.75rem, 4vw, 1.5rem);
      }

      footer {
        padding: 2.5rem 1rem 3rem;
        text-align: center;
        color: #7b756d;
        font-size: 0.85rem;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 1.5rem auto;
        border-radius: 12px;
      }

      pre {
        background: rgba(26, 26, 26, 0.05);
        padding: 1rem;
        border-radius: 12px;
        overflow-x: auto;
      }

      code {
        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
        font-size: 0.95em;
      }

      blockquote {
        border-left: 3px solid rgba(40, 86, 182, 0.35);
        margin: 1.5rem 0;
        padding-left: 1rem;
        color: #5f5b55;
      }

      .post {
        margin-bottom: 3.5rem;
      }

      .post-header h1 {
        margin: 0 0 0.5rem;
        font-size: 2.1rem;
        letter-spacing: -0.015em;
      }

      .post-meta {
        color: #6d675f;
        font-size: 0.92rem;
      }

      .post-content p:first-of-type {
        margin-top: 0;
      }

      .post-nav {
        margin-top: 3rem;
      }

      .post-nav a {
        font-weight: 500;
      }

      .post-list {
        list-style: none;
        margin: 0;
        padding: 0;
      }

      .post-list li {
        padding: 1.6rem 0;
        border-bottom: 1px solid rgba(31, 31, 31, 0.08);
      }

      .post-list h2 {
        margin: 0;
        font-size: 1.45rem;
        letter-spacing: -0.01em;
      }

      .post-list .post-meta {
        margin: 0.45rem 0 0;
      }

      .post-list p {
        margin: 0.7rem 0 0;
        color: #5f5b55;
      }

      .post-list li:last-child {
        border-bottom: none;
      }

      .hero {
        margin: 3rem 0 2.5rem;
      }

      .hero-content {
        max-width: 660px;
      }

      .hero h1 {
        font-size: clamp(2.2rem, 4vw, 2.8rem);
        margin-bottom: 0.35rem;
        letter-spacing: -0.02em;
      }

      .hero-lead {
        font-size: 1.18rem;
        margin: 0 0 0.75rem;
        color: #2b2926;
      }

      .hero-subtext {
        color: #6c635a;
        margin: 0;
        line-height: 1.6;
      }

      .post-links {
        display: inline-flex;
        align-items: center;
        gap: 1rem;
        margin-top: 0.9rem;
        font-size: 0.95rem;
      }

      .post-links a {
        display: inline-flex;
        align-items: center;
        gap: 0.4rem;
        text-decoration: underline;
        color: inherit;
      }

      .post-links .code-icon {
        width: 16px;
        height: 16px;
        flex-shrink: 0;
      }

      @media (max-width: 540px) {
        header {
          padding: 2rem 1rem 1.25rem;
        }

        header h1 {
          font-size: 1.6rem;
        }

        .post-header h1 {
          font-size: 1.8rem;
        }

        main {
          width: min(640px, calc(100% - 1.5rem));
        }

        .post-links {
          gap: 0.75rem;
        }
      }
    </style>
  </head>
  <body>
    <header>
      <div class="site-identity">
        <h1><a href="/gautierevn/">Gautier Evennou</a></h1>
        <p class="site-tagline">PhD blog on computer vision and academic life.</p>
      </div>
    </header>

    <main>
      <article class="post">
  <header class="post-header">
    <h1>WACV 2025</h1>
    <p class="post-meta">
      <time datetime="2025-10-06T00:00:00+02:00">October 6, 2025</time>
       · Gautier Evennou
    </p>
  </header>

  <section class="post-content">
    <h1 id="wacv-2025">WACV 2025</h1>

<p><strong>Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation</strong> <a href="https://arxiv.org/abs/2412.15939">https://arxiv.org/abs/2412.15939</a></p>

<p>Oral Paper Round 1 Acceptance (for those who care about this)</p>

<p>Ok, a bit late to the party but i wanted to share the first work of my thesis.</p>

<p>I work on semantic alteration in multimedia content (whatever this is), and we wanted to see if we could compare two images, one original and one edited. Then came InstructPix2Pix, a milestone in image editing models ! We thought “Hey, we got this cool model to generate new version of an image, it would be nice to use it for synthetic data as it is underexplored in computer vision”. Obviously, since then a lot of work has been made on synthetic data but at this time it was pretty new.</p>

<p><img src="/gautierevn/assets/images/ip2p_mainframe.jpeg" alt="InstructPix2Pix, Learning to Follow Image Editing Instructions, Tim Brooks et. al 2022" /></p>

<p>InstructPix2Pix, Learning to Follow Image Editing Instructions, Tim Brooks et. al 2022</p>

<p>With this fresh idea in my mind, we started to think about how to compare images. The most streamlined way of doing this is to take two images independently, extract their respective features and then somehow merge them so that a decoder output a “difference caption”. We saw that this was rather standard in IDC and Change Captioning. Then, we picked BLIP2, state-of-the-art image captioning model that was trained with synthetic data to generate better caption.</p>

<table>
  <tbody>
    <tr>
      <td>First hurdle, GPU access. At the beginning i only had access to one 4090 so i had to find a way to finetune BLIP2(2.7B) in an efficient manner. Then LoRA enters the chat and basically singlehandledly saves this paper. For those unfamiliar with LoRA, it stands for LowRankAdaption. It consists in freezing the whole model BLIP2, then replacing each linear layer with dim $k<em>d$ by two matrices A and B of dim $k</em>r</td>
      <td>r*d$. In practice, we use a rank of 8,16, or 32 but no more. This accounts for a very efficient finetuning, as only 0.01% of the parameters of the original model are updated. Then only the LoRA matrices are saved and can be loaded into BLIP2 at inference. We finetune linear layer from transformers layers a.k.a the q,k,v matrices.</td>
    </tr>
  </tbody>
</table>

<p>Now that we can finetune, using the classic pipeline of parallel feature extraction was performing pretty bad. Subpar to state-of-the-art method. Hum, strange right ? Our intuition on this is a classic lost in translation mechanism. By projecting features to a high dimensional space, they are apart from each other intrinsically (see the curse of dimensionality). Then we thought how to bypass this problem. How would you do it ? An intuitive way of solving this problem is to use anthropomorphism, “how would human compare two images” ? I guess it is no surprise that a sound human would put them side-by-side to limit noise due to unrelated information in his environment. Ok, let’s try it right ? Side-by-side it basically concatenation in the image space, so our BLIP2 will now take a $256²$ image made of two $128,256$ images. (vertical concatenation). And then we go through BLIP2 flow of image extraction, QFormer and text decoder. This idea of joint encoding does makes sense as we do not want to represent images and compare their representation as it requires exhaustive representation, we rather want to straightforwardly represent their differences as it is our main objective.</p>

<p>Ok cool, basically just by using BLIP2 and this joint encoding mechanism we got state-of-the-art in 20 minutes using LoRA ! Not so bad right ? Well it was only possible because in the IDC field most people were focused on designing super smart modules with convoluted math <del>to ensure acceptance at prestigious conference</del>. We did not optimize for that. More importantly, we tried to nudge the field to be more practically oriented, like leveraging heavily pretrained VLMs instead of doing another training from scratch to earn 1% of performance. By that we mean solving two main issues of the IDC task: <em>knowledge of the visual world</em> and <em>data scarcity</em>. VLMs learned in the hard way the visual world, understanding relationships and nature of the objects in a scenery. But now, to adapt them to IDC, which biggest dataset comprises less than 3000 images, it is another story. Efficient Finetuning helps; LoRA tends to learn really fast and with few data but more quality data is the easiest path to success. That’s why we advocate for synthetic augmentation.</p>

<p>Data augmentation is well-know, i don’t think i need to introduce it here. Synthetic augmentation is more recent in the computer vision field; we leverage recent generative models as a way to produce quality data most aligned with our objective, hopefully without compromising with generalization to real world data. This has been used for years in the Natural Language Processing(NLP) field, which deals with text modality. But for image it is really the new generative paradigm, fueled by the release of StableDiffusion in 2022, which unlocked this approach. InstructPix2Pix generates text data using GPT3 and new versions of an image using those prompts. By doing so they train a new model, IP2P which is able to edit existing images. This work paved the way to many more, mainly led by the chinese industry (bytedance, alibaba, tencent). So now we got a perfect model to generate data to finetune our BLIP2 on, isn’t it great ?</p>

<p>We conducted many experiments, namely to see if synthetic data alone is enough to achieve good performances and reach a pretty cool conclusion that yes, synthetic data are enough to adapt BLIP2 to IDC and reach SOTA performances.</p>

<p>Here are some results to convince some of you.</p>

<p>Now that we have a super cool model and a cool way of gathering data, is there anything more to say ? In fact yes. But it has more to do with academic benchmarks and how flawed they are. I wont speak about goodhart law or criticize metrics. What i will say is that a sentence, trained to be the mean one using a train set of any dataset of IDC, is better than some methods published at prestigious conferences. Meaning that they basically do not output anything more significant than the most likely sentence that has no accept to input images. So yeah, there is that 😕 Moreover, NLP metrics such as BLEU4 or CIDEr need at least 5 references (ground-truths) to be meaningful. Most IDC datasets does not provide as much references as needed. We try to bypass this issue and make a more robust evaluation by prompting LLAMA 2 to generate variations of the reference. I am not a big fan to “put your problem in an LLM and paper”. Yeah it is clunky, yeah it introduces bias but it does make more sense and is more aligned with the spirit of the metrics so please forgive us. So we hope our results are more robust to.</p>

<p>Now, one more thing: here are some cool visualizations of attention maps from our model when it performs comparison between images. We basically see where it looks to take a decision and generate the difference caption. This could be very useful for explaining the difference caption, like telling “here is the difference, and here is the description of this difference”.</p>

<p>I wont dive into all the details of the paper, the goal of this blogpost is more to give you insights into my thought process and in the paper intuitions.</p>

<p>Having said that, i do not feel there is much to do in this task of IDC. The VLMs performance is skyrocketing this recent years and i expect them to perform better with time. Then, these saturated and flawed benchmarks we got wont be able to correctly assess significant improvements.</p>

<p>Also, this task is stringent by design for real-world applications, as it requires access to the edited image AND the original one. Would be cool to solve the same issue without the need for the original image right ? If you think so, have a look to my subsequent works, SWIFT : Semantic Watermarking for Image Forgery Thwarting and Fast, Secure an blabla. Those works, while being on different tasks than IDC, try to address the same issue.</p>

<p><img src="WACV%202025%20284383abbeb4803a8087e7ae44c3450e/attention_examples.jpeg" alt="Difference captioning attention examples" /></p>

  </section>

  <nav class="post-nav">
    <a href="/gautierevn/">← Back to all posts</a>
  </nav>
</article>

    </main>

    <footer>
      <small>&copy; 2025 PhD blog</small>
    </footer>
  </body>
</html>
