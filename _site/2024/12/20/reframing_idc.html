<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" type="image/svg+xml" href="/gautierevn/assets/images/gpu-favicon_v2.svg">
    <title>Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation - Gautier Evennou</title>
    <style>
      :root {
        color-scheme: light;
      }

      *, *::before, *::after {
        box-sizing: border-box;
      }

      body {
        margin: 0;
        font-family: "Inter", "Segoe UI", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
        background: #f6f3ed;
        color: #1f1f1f;
        line-height: 1.7;
        text-rendering: optimizeLegibility;
      }

      a {
        color: #2856b6;
        text-decoration: none;
        transition: color 0.2s ease;
      }

      a:hover {
        color: #1c3f83;
        text-decoration: underline;
      }

      

      header {
        padding: 2.25rem clamp(1rem, 4vw, 3rem);
        border-bottom: 1px solid rgba(31, 31, 31, 0.07);
        background: #fbf7f0;
        display: flex;
        justify-content: space-between;
        align-items: baseline;
        gap: 1rem;
        flex-wrap: wrap;
        max-width: 960px;
        margin: 0 auto;
      }

      header h1,
      header .site-tagline {
        margin: 0;
      }

      header {
        padding: 2.25rem clamp(1rem, 4vw, 3rem);
        border-bottom: 1px solid rgba(31, 31, 31, 0.07);
        background: #fbf7f0;
        display: flex;
        justify-content: space-between;
        align-items: baseline;
        gap: 1rem;
        flex-wrap: wrap;
        max-width: 960px;
        margin: 0 auto;
      }

      header h1 {
        margin: 0;
        font-size: 1.5rem;
        font-weight: 600;
        letter-spacing: -0.01em;
        color: #1f3f9a;
      }

      header .site-tagline {
        margin: 0;
        color: #615b54;
        font-size: 0.95rem;
      }

      main {
        width: min(780px, calc(100% - 3rem));
        margin: 0 auto 4rem;
        padding: 0 clamp(0.75rem, 4vw, 1.5rem);
      }

      footer {
        padding: 2.5rem 1rem 3rem;
        text-align: center;
        color: #7b756d;
        font-size: 0.85rem;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 1.5rem auto;
        border-radius: 12px;
      }

      pre {
        background: rgba(26, 26, 26, 0.05);
        padding: 1rem;
        border-radius: 12px;
        overflow-x: auto;
      }

      code {
        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
        font-size: 0.95em;
      }

      blockquote {
        border-left: 3px solid rgba(40, 86, 182, 0.35);
        margin: 1.5rem 0;
        padding-left: 1rem;
        color: #5f5b55;
      }

      .post {
        margin-bottom: 3.5rem;
        background: inherit;
        padding: 0;
        box-shadow: none;
        border: none;
      }

      .post-header h1 {
        margin: 0 0 0.5rem;
        font-size: 1.85rem;
        letter-spacing: -0.015em;
      }

      .post-meta {
        color: #6d675f;
        font-size: 0.92rem;
      }

      .post-tags {
        list-style: none;
        margin: 0.6rem 0 0;
        padding: 0;
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
      }

      .post-tags li {
        background: rgba(40, 86, 182, 0.12);
        color: #1f3f9a;
        font-size: 0.82rem;
        padding: 0.2rem 0.6rem;
        border-radius: 999px;
      }

      .post-content p:first-of-type {
        margin-top: 0;
      }

      .post-nav {
        margin-top: 3rem;
      }

      .post-nav a {
        font-weight: 500;
      }

      .post-list {
        list-style: none;
        margin: 0;
        padding: 0;
      }

      .post-list li {
        padding: 1.6rem 0;
        border-bottom: 1px solid rgba(31, 31, 31, 0.08);
        background: inherit;
      }

      .post-list h2 {
        margin: 0;
        font-size: 1.45rem;
        letter-spacing: -0.01em;
      }

      .post-list .post-meta {
        margin: 0.45rem 0 0;
      }

      .post-list p {
        margin: 0.7rem 0 0;
        color: #5f5b55;
      }

      .post-list li:last-child {
        border-bottom: none;
      }

      .hero {
        margin: 3.5rem 0 2.5rem;
      }

      .hero-kicker {
        font-size: 0.95rem;
        text-transform: uppercase;
        letter-spacing: 0.18em;
        color: #5b71c7;
        margin-bottom: 0.75rem;
        display: inline-block;
      }

      .hero h1 {
        font-size: clamp(2.25rem, 4vw, 2.9rem);
        margin-bottom: 0.4rem;
        letter-spacing: -0.02em;
      }

      .hero-summary {
        font-size: 1.1rem;
        color: #3a3530;
        margin: 0;
      }

      .about {
        margin: 0 0 3.5rem;
      }

      .about-layout {
        display: grid;
        gap: clamp(1.8rem, 5vw, 3.2rem);
        grid-template-columns: minmax(0, 1fr);
        align-items: start;
      }

      .about-summary h2 {
        margin: 0 0 1rem;
        font-size: 1.45rem;
      }

      .about-summary p {
        margin: 0 0 1rem;
        color: #3e3a35;
      }

      .post-links {
        display: inline-flex;
        align-items: center;
        gap: 1rem;
        margin-top: 0.9rem;
        font-size: 0.95rem;
      }

      .post-links a {
        display: inline-flex;
        align-items: center;
        gap: 0.4rem;
        text-decoration: underline;
        color: inherit;
      }

      .post-links .code-icon {
        width: 16px;
        height: 16px;
        flex-shrink: 0;
      }

      .post-links .paper-icon {
        height: 16px;
        width: auto;
        flex-shrink: 0;
      }

      @media (max-width: 540px) {
        header {
          padding: 2rem 1rem 1.25rem;
        }

        header h1 {
          font-size: 1.6rem;
        }

        .post-header h1 {
          font-size: 1.8rem;
        }

        main {
          width: min(640px, calc(100% - 1.5rem));
        }

        .post-links {
          gap: 0.75rem;
        }

        .hero {
          margin: 3rem 0 2rem;
        }

        .about-layout {
          grid-template-columns: 1fr;
        }

      }
    </style>
  </head>
  <body>
    <header>
      <h1><a href="/gautierevn/">Gautier Evennou</a></h1>
      <p class="site-tagline">PhD blog on computer vision and academic life.</p>
    </header>

    <main>
      <article class="post">
  <header class="post-header">
    <h1>Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation</h1>
    <p class="post-meta">
      <time datetime="2024-12-20T00:00:00+01:00">December 20, 2024</time>
       · Gautier Evennou, Antoine Chaffin, Vivien Chappelier, and Ewa Kijak
    </p>
    
    <ul class="post-tags">
      
        
        
        <li>IEEE WACV 2025</li>
      
    </ul>
    
  </header>

  <section class="post-content">
    <p>Oral Paper Round 1 Acceptance (for those who track such things)</p>

<p>I am a bit late to the party, but I wanted to share the first publication of my PhD.</p>

<p>I work on semantic alteration in multimedia content and we wanted to compare two images: one original and one edited. Then came InstructPix2Pix, a milestone in image editing models. We thought “this model generates new versions of an image; it would be nice to use it for synthetic data because that space is underexplored in computer vision.” Since then synthetic data has exploded, but at that time it was fairly new.</p>

<p><img src="/gautierevn/assets/images/ip2p_mainframe.jpeg" alt="InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022)" />
<em>Figure 1. InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022).</em></p>

<p>With this fresh idea, we started to think about how to compare images. The streamlined approach was to take two images independently, extract their features, merge them, and let a decoder output a difference caption. That was standard practice in IDC and change captioning. We picked BLIP2, a state-of-the-art image captioning model trained with synthetic data to generate better captions.</p>

<p>First hurdle: GPU access. At the beginning I had a single 4090, so I needed an efficient way to fine-tune BLIP2 (2.7B). Enter LoRA, which basically saved the paper. LoRA freezes the base model and swaps each linear layer with two low-rank matrices. In practice we use ranks 8, 16, or 32 - no more. You end up updating only a tiny fraction of the parameters, saving the adapters, and reloading them at inference. We fine-tune the transformer layers - the q, k, and v projections.</p>

<p>Once we could fine-tune, the classic parallel feature extraction pipeline underperformed. It was below state-of-the-art. Our intuition: this is the curse of dimensionality. By projecting features to high dimensions, representations drift apart. We wanted to bypass that. How do humans compare images? We place them side by side. So we tried the same: we concatenated the two images vertically and let BLIP2 process a joint 256x256 input. That joint encoding is the key - we do not want separate exhaustive representations; we want to represent differences directly.</p>

<p>With BLIP2 and joint encoding we reached state-of-the-art in about 20 minutes of LoRA fine-tuning. Not bad. It worked partly because most IDC papers focused on elaborate modules instead of leveraging large pretrained VLMs. We tried to nudge the field toward practical setups: pretrained models provide visual world knowledge, we add efficient finetuning plus synthetic augmentation to overcome data scarcity.</p>

<p>Synthetic augmentation was still new on the vision side. InstructPix2Pix generates text variations with GPT-3 and edited images from prompts, giving a controllable generator for data creation. We explored how far synthetic data alone can take BLIP2 for IDC and found it can reach SOTA performance.</p>

<p>Benchmarks, however, are flawed. Some methods barely outperform a constant caption (the mean sentence of a dataset). Many NLP metrics like BLEU4 or CIDEr need several references to be meaningful, yet most IDC datasets provide fewer than five. We tried to mitigate this by prompting Llama 2 to generate reference variations. It is not perfect, but it aligns better with what those metrics expect.</p>

<p>We also looked at attention maps to explain where the model focuses when producing a difference caption. That helps to justify the generated sentence.</p>

<p>IDC benchmarks are close to saturation and the task itself is constrained - it assumes access to both original and edited images. Real deployments might not have that luxury. If you care about that scenario, check my follow-up works such as SWIFT (Semantic Watermarking for Image Forgery Thwarting).</p>

<p><img src="WACV%202025%20284383abbeb4803a8087e7ae44c3450e/attention_examples.jpeg" alt="Difference captioning attention examples" /></p>

  </section>

  <nav class="post-nav">
    <a href="/gautierevn/">← Back to all posts</a>
  </nav>
</article>
    </main>

    <footer>
      <small>&copy; 2025 PhD blog</small>
    </footer>
  </body>
</html>





