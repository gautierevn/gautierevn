<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/gautierevn/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gautierevn/" rel="alternate" type="text/html" /><updated>2025-10-13T13:34:17+02:00</updated><id>http://localhost:4000/gautierevn/feed.xml</id><title type="html">Gautier Evennou</title><subtitle>PhD blog on computer vision and academic life.</subtitle><entry><title type="html">Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors</title><link href="http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking.html" rel="alternate" type="text/html" title="Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors" /><published>2025-10-01T00:00:00+02:00</published><updated>2025-10-01T00:00:00+02:00</updated><id>http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking</id><content type="html" xml:base="http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking.html"><![CDATA[<h2 id="highlights">Highlights</h2>

<ul>
  <li>Encode natural-language prompts into robust watermark vectors that survive typical sharing pipelines.</li>
  <li>Balance payload size and secrecy with an adversarial training loop that keeps the watermark imperceptible.</li>
  <li>Deliver a fast inference path compatible with modern diffusion systems for large-scale provenance tracking.</li>
</ul>

<p>Code will be published alongside the camera-ready release, stay tuned.</p>]]></content><author><name>Gautier Evennou, Vivien Chappelier, and Ewa Kijak</name></author><category term="watermarking" /><category term="image-forensics" /><category term="robustness" /><summary type="html"><![CDATA[Autoencode textual payloads into invisible signals that stay resilient to edits while carrying high-capacity provenance.]]></summary></entry><entry><title type="html">Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation</title><link href="http://localhost:4000/gautierevn/2024/12/20/reframing_idc.html" rel="alternate" type="text/html" title="Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation" /><published>2024-12-20T00:00:00+01:00</published><updated>2024-12-20T00:00:00+01:00</updated><id>http://localhost:4000/gautierevn/2024/12/20/reframing_idc</id><content type="html" xml:base="http://localhost:4000/gautierevn/2024/12/20/reframing_idc.html"><![CDATA[<p>Oral Paper Round 1 Acceptance (for those who track such things)</p>

<p>I am a bit late to the party, but I wanted to share the first publication of my PhD.</p>

<p>I work on semantic alteration in multimedia content and we wanted to compare two images: one original and one edited. Then came InstructPix2Pix, a milestone in image editing models. We thought “this model generates new versions of an image; it would be nice to use it for synthetic data because that space is underexplored in computer vision.” Since then synthetic data has exploded, but at that time it was fairly new.</p>

<p><img src="/gautierevn/assets/images/ip2p_mainframe.jpeg" alt="InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022)" />
<em>Figure 1. InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022).</em></p>

<p>With this fresh idea, we started to think about how to compare images. The streamlined approach was to take two images independently, extract their features, merge them, and let a decoder output a difference caption. That was standard practice in IDC and change captioning. We picked BLIP2, a state-of-the-art image captioning model trained with synthetic data to generate better captions.</p>

<p>First hurdle: GPU access. At the beginning I had a single 4090, so I needed an efficient way to fine-tune BLIP2 (2.7B). Enter LoRA, which basically saved the paper. LoRA freezes the base model and swaps each linear layer with two low-rank matrices. In practice we use ranks 8, 16, or 32 - no more. You end up updating only a tiny fraction of the parameters, saving the adapters, and reloading them at inference. We fine-tune the transformer layers - the q, k, and v projections.</p>

<p>Once we could fine-tune, the classic parallel feature extraction pipeline underperformed. It was below state-of-the-art. Our intuition: this is the curse of dimensionality. By projecting features to high dimensions, representations drift apart. We wanted to bypass that. How do humans compare images? We place them side by side. So we tried the same: we concatenated the two images vertically and let BLIP2 process a joint 256x256 input. That joint encoding is the key - we do not want separate exhaustive representations; we want to represent differences directly.</p>

<p>With BLIP2 and joint encoding we reached state-of-the-art in about 20 minutes of LoRA fine-tuning. Not bad. It worked partly because most IDC papers focused on elaborate modules instead of leveraging large pretrained VLMs. We tried to nudge the field toward practical setups: pretrained models provide visual world knowledge, we add efficient finetuning plus synthetic augmentation to overcome data scarcity.</p>

<p>Synthetic augmentation was still new on the vision side. InstructPix2Pix generates text variations with GPT-3 and edited images from prompts, giving a controllable generator for data creation. We explored how far synthetic data alone can take BLIP2 for IDC and found it can reach SOTA performance.</p>

<p>Benchmarks, however, are flawed. Some methods barely outperform a constant caption (the mean sentence of a dataset). Many NLP metrics like BLEU4 or CIDEr need several references to be meaningful, yet most IDC datasets provide fewer than five. We tried to mitigate this by prompting Llama 2 to generate reference variations. It is not perfect, but it aligns better with what those metrics expect.</p>

<p>We also looked at attention maps to explain where the model focuses when producing a difference caption. That helps to justify the generated sentence.</p>

<p>IDC benchmarks are close to saturation and the task itself is constrained - it assumes access to both original and edited images. Real deployments might not have that luxury. If you care about that scenario, check my follow-up works such as SWIFT (Semantic Watermarking for Image Forgery Thwarting).</p>

<p><img src="WACV%202025%20284383abbeb4803a8087e7ae44c3450e/attention_examples.jpeg" alt="Difference captioning attention examples" /></p>]]></content><author><name>Gautier Evennou, Antoine Chaffin, Vivien Chappelier, and Ewa Kijak</name></author><category term="conference:IEEE WACV 2025" /><summary type="html"><![CDATA[Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation.]]></summary></entry><entry><title type="html">SWIFT: Semantic Watermarking for Image Forgery Thwarting</title><link href="http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking.html" rel="alternate" type="text/html" title="SWIFT: Semantic Watermarking for Image Forgery Thwarting" /><published>2024-07-15T00:00:00+02:00</published><updated>2024-07-15T00:00:00+02:00</updated><id>http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking</id><content type="html" xml:base="http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking.html"><![CDATA[<h2 id="tldr">TL;DR</h2>

<ul>
  <li>Embed a semantic watermark during editing so post-hoc detectors can localize tampering.</li>
  <li>Keep perceptual quality intact on pristine assets by injecting the signal only when an edit happens.</li>
  <li>Release an open pipeline (paper + code) to reproduce benchmarks and ablations.</li>
</ul>

<h2 id="why-swift">Why SWIFT?</h2>

<p>Semantic alteration is now cheap, but most forensic tools struggle to tell genuine stories from fabricated ones. Rather than guessing after the fact, SWIFT (Semantic Watermarking for Image Forgery Thwarting) nudges the generator to stamp an editable image with an imperceptible yet verifiable signal. The idea is simple: if we cooperate with the editing network, downstream auditors can later check authenticity without needing the original asset.</p>

<h2 id="how-it-works">How it works</h2>

<ol>
  <li><strong>Watermark guidance</strong> - while editing an image, we optimize a frequency-aware watermark loss so the edit carries a semantic signature.</li>
  <li><strong>Tamper localization</strong> - a lightweight detector recovers the watermark and highlights which pixels were modified.</li>
  <li><strong>Integrity switch</strong> - if an image is untouched, we skip the watermark step, so there is no quality hit on clean content.</li>
</ol>

<p>The repository includes training scripts, evaluation notebooks, and ablation studies on robustness against compression, resizing, and intentional removal attempts.</p>

<h2 id="whats-next">What’s next</h2>

<p>We are exploring how SWIFT pairs with generative provenance systems, and whether similar signals can travel across video timelines. Feedback and pull requests are welcome - grab the code, run the benchmarks, and let me know where to improve.</p>]]></content><author><name>Gautier Evennou, Vivien Chappelier and Ewa Kijak</name></author><category term="conference:IEEE WIFS 2024" /><category term="watermarking" /><category term="image-forensics" /><category term="reliability" /><summary type="html"><![CDATA[Watermark semantic edits so detectors can flag tampered images without hurting clean media.]]></summary></entry></feed>