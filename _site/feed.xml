<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/gautierevn/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/gautierevn/" rel="alternate" type="text/html" /><updated>2025-10-13T15:51:06+02:00</updated><id>http://localhost:4000/gautierevn/feed.xml</id><title type="html">Gautier Evennou</title><subtitle>PhD blog on computer vision and academic life.</subtitle><entry><title type="html">Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors</title><link href="http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking.html" rel="alternate" type="text/html" title="Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors" /><published>2025-10-01T00:00:00+02:00</published><updated>2025-10-01T00:00:00+02:00</updated><id>http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking</id><content type="html" xml:base="http://localhost:4000/gautierevn/2025/10/01/fast-secure-high-capacity-watermarking.html"><![CDATA[<h2 id="highlights">Highlights</h2>

<ul>
  <li>Encode natural-language prompts into robust watermark vectors that survive typical sharing pipelines.</li>
  <li>Balance payload size and secrecy with an adversarial training loop that keeps the watermark imperceptible.</li>
  <li>Deliver a fast inference path compatible with modern diffusion systems for large-scale provenance tracking.</li>
</ul>

<p>Code will be published alongside the camera-ready release, stay tuned.</p>]]></content><author><name>Gautier Evennou, Vivien Chappelier, and Ewa Kijak</name></author><category term="conference:arXiv Preprint" /><category term="watermarking" /><category term="image-forensics" /><category term="robustness" /><summary type="html"><![CDATA[Designing text autoencoders to craft a latent space suitable for secure watermarking]]></summary></entry><entry><title type="html">Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation</title><link href="http://localhost:4000/gautierevn/2024/12/20/reframing_idc.html" rel="alternate" type="text/html" title="Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation" /><published>2024-12-20T00:00:00+01:00</published><updated>2024-12-20T00:00:00+01:00</updated><id>http://localhost:4000/gautierevn/2024/12/20/reframing_idc</id><content type="html" xml:base="http://localhost:4000/gautierevn/2024/12/20/reframing_idc.html"><![CDATA[<p>Oral Paper Round 1 Acceptance (for those who track such things)</p>

<p>I am a bit late to the party, but I wanted to share the first publication of my PhD.</p>

<p>I work on semantic alteration in multimedia content and we wanted to compare two images: one original and one edited. Then came InstructPix2Pix, a milestone in image editing models. We thought “this model generates new versions of an image; it would be nice to use it for synthetic data because that space is underexplored in computer vision.” Since then synthetic data has exploded, but at that time it was fairly new.</p>

<p><img src="/gautierevn/assets/images/ip2p_mainframe.jpeg" alt="InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022)" />
<em>Figure 1. InstructPix2Pix — Learning to Follow Image Editing Instructions (Brooks et al., 2022).</em></p>

<p>With this fresh idea, we started to think about how to compare images. The streamlined approach was to take two images independently, extract their features, merge them, and let a decoder output a difference caption. That was standard practice in IDC and change captioning. We picked BLIP2, a state-of-the-art image captioning model trained with synthetic data to generate better captions.</p>

<p>First hurdle: GPU access. At the beginning I had a single 4090, so I needed an efficient way to fine-tune BLIP2 (2.7B). Enter LoRA, which basically saved the paper. LoRA freezes the base model and swaps each linear layer with two low-rank matrices. In practice we use ranks 8, 16, or 32 - no more. You end up updating only a tiny fraction of the parameters, saving the adapters, and reloading them at inference. We fine-tune the transformer layers - the q, k, and v projections.</p>

<p>Once we could fine-tune, the classic parallel feature extraction pipeline underperformed. It was below state-of-the-art. Our intuition: this is the curse of dimensionality. By projecting features to high dimensions, representations drift apart. We wanted to bypass that. How do humans compare images? We place them side by side. So we tried the same: we concatenated the two images vertically and let BLIP2 process a joint 256x256 input. That joint encoding is the key - we do not want separate exhaustive representations; we want to represent differences directly.</p>

<p>With BLIP2 and joint encoding we reached state-of-the-art in about 20 minutes of LoRA fine-tuning. Not bad. It worked partly because most IDC papers focused on elaborate modules instead of leveraging large pretrained VLMs. We tried to nudge the field toward practical setups: pretrained models provide visual world knowledge, we add efficient finetuning plus synthetic augmentation to overcome data scarcity.</p>

<p>Synthetic augmentation was still new on the vision side. InstructPix2Pix generates text variations with GPT-3 and edited images from prompts, giving a controllable generator for data creation. We explored how far synthetic data alone can take BLIP2 for IDC and found it can reach SOTA performance.</p>

<p>Benchmarks, however, are flawed. Some methods barely outperform a constant caption (the mean sentence of a dataset). Many NLP metrics like BLEU4 or CIDEr need several references to be meaningful, yet most IDC datasets provide fewer than five. We tried to mitigate this by prompting Llama 2 to generate reference variations. It is not perfect, but it aligns better with what those metrics expect.</p>

<p>We also looked at attention maps to explain where the model focuses when producing a difference caption. That helps to justify the generated sentence.</p>

<p>IDC benchmarks are close to saturation and the task itself is constrained - it assumes access to both original and edited images. Real deployments might not have that luxury. If you care about that scenario, check my follow-up works such as SWIFT (Semantic Watermarking for Image Forgery Thwarting).</p>

<p><img src="WACV%202025%20284383abbeb4803a8087e7ae44c3450e/attention_examples.jpeg" alt="Difference captioning attention examples" /></p>]]></content><author><name>Gautier Evennou, Antoine Chaffin, Vivien Chappelier, and Ewa Kijak</name></author><category term="conference:IEEE WACV 2025" /><category term="presentation:Oral" /><summary type="html"><![CDATA[Using VLM and synthetic data to improve Image Difference Captioning]]></summary></entry><entry><title type="html">SWIFT: Semantic Watermarking for Image Forgery Thwarting</title><link href="http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking.html" rel="alternate" type="text/html" title="SWIFT: Semantic Watermarking for Image Forgery Thwarting" /><published>2024-07-15T00:00:00+02:00</published><updated>2024-07-15T00:00:00+02:00</updated><id>http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking</id><content type="html" xml:base="http://localhost:4000/gautierevn/2024/07/15/swift-semantic-watermarking.html"><![CDATA[<h2 id="tldr">TL;DR</h2>

<ul>
  <li>Embed a semantic watermark so post-hoc detectors can assert authenticity.</li>
  <li>Keep perceptual quality intact on pristine assets by injecting the signal imperceptibly.</li>
  <li>Release an open pipeline (paper + code) to reproduce benchmarks and ablations.</li>
</ul>

<h2 id="why-swift">Why SWIFT?</h2>

<p>Semantic alteration is now cheap, with image editing leading the charge. Most forensic tools struggle to tell genuine stories from fabricated ones, the best being only able to tell is a specific AI generator was used. Rather than guessing after the fact, SWIFT (Semantic Watermarking for Image Forgery Thwarting) embed the meaning of the image by projecting a caption into an imperceptible yet verifiable signal. The idea is simple: if we know what the image was using the caption, downstream auditors can later check authenticity without needing the original asset.</p>

<h2 id="how-it-works">How it works</h2>

<ol>
  <li><strong>Caption</strong> - a captioning model is responsible for providing a textual description of the image.</li>
  <li><strong>Real Vector watermarking</strong> - Usually, deep-learning based watermarking models embed bits, not real vectors. We chose this path as it enables us to carry more information.</li>
  <li><strong>Confidence Metric</strong> - at the receiving end, a confidence metric is computed to know is the decoded message is close from its perfect representation. This metric is heavily correlated with correctness and ensure the trustworthiness of the watermark.</li>
</ol>

<h2 id="whats-next">What’s next</h2>

<p>SWIFT was really fun to write as it introduced a new idea in the field of watermarking, using it as a communication channel. But the capacity is rather limited, up to 48 bits and then the interferences from the modulation become to much of a burden. So next work will focus on trying to bypass this modulation that is responsible for projecting bits into a real-valued vector.</p>]]></content><author><name>Gautier Evennou, Vivien Chappelier and Ewa Kijak</name></author><category term="conference:IEEE WIFS 2024" /><category term="presentation:Oral" /><category term="watermarking" /><category term="image-forensics" /><category term="reliability" /><summary type="html"><![CDATA[Watermark semantic edits so detectors can flag tampered images without hurting clean media.]]></summary></entry></feed>